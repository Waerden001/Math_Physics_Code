{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Characteristic functions, correlations and indenpendency.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nopsMp5g_1Ux",
        "colab_type": "text"
      },
      "source": [
        "# Characteristic functions, correlations and independency\n",
        "\n",
        "We make precise via characteristic functions the impression that\n",
        "\n",
        "$$\\text{Zero correlation will indicate no linear dependency, however won't capture non-linearity.}$$\n",
        "\n",
        "---\n",
        "(Example) $X$ is a uniformly distributed random variable on $[-1,1]$, then $X$ and $X^2$ have zero correlation, but apparently **not** independent.\n",
        "\n",
        "---\n",
        "We want to understand the tayler expansion of the Fourier transform of the joint distribution, namely \n",
        "$$\\widehat{F}(u,v)=\\widehat{F}(0,0)+\\frac{\\partial \\widehat{F}(0,0)}{\\partial u}u + \\frac{\\partial \\widehat{F}(0,0)}{\\partial v}v +\\frac{1}{2}\\begin{bmatrix}u,v\\end{bmatrix} \\begin{bmatrix}\\frac{\\partial^{2}\\widehat{F}(0,0)}{\\partial u^2} & \\frac{\\partial^{2}\\widehat{F}(0,0)}{\\partial uv}\\\\\n",
        "\\frac{\\partial^{2}\\widehat{F}(0,0)}{\\partial uv} & \\frac{\\partial^{2}\\widehat{F}(0,0)}{\\partial v^2}\\end{bmatrix}\\begin{bmatrix}u\\\\v\\end{bmatrix}+o(|u^2+v^2|).$$\n",
        "\n",
        "---\n",
        "In the terminology above, we have \n",
        "$$\\begin{align*}\\widehat{F}(0,0)&=1\\\\\\frac{\\partial \\widehat{F}(0,0)}{\\partial u} &= iE[X]\\\\\n",
        "\\frac{\\partial \\widehat{F}(0,0)}{\\partial v} &= iE[Y]\\\\\n",
        "\\frac{\\partial^{2}\\widehat{F}(0,0)}{\\partial u^2}&=iE[X^2]\\\\\n",
        "\\frac{\\partial^{2}\\widehat{F}(0,0)}{\\partial v^2}&=iE[Y^2]\\\\\n",
        "\\frac{\\partial^{2}\\widehat{F}(0,0)}{\\partial uv}&=i^2E[XY]=-E[XY]\\end{align*}$$\n",
        "---\n",
        "\n",
        "Easily, if $X$ and $Y$ are indenpendent, we have $F(x,y)=f(x)g(y)$ \n",
        "\n",
        "$$\\widehat{F}=E[e^{iuX}e^{ivY}]=\\int e^{iux}e^{ivy}F(x,y)dxdy=\\int e^{iux}e^{ivy}f(x)g(y)dxdy=E[e^{iuX}]E[e^{ivY}]$$\n",
        "In this case, we have the Taylor expansion\n",
        "$$\\begin{align*}\\widehat{F}(0,0)&=(1+ iE[X]u+\\frac{1}{2}i^2E[X^2]u^2+o(u^2))(1+ iE[X]u+\\frac{1}{2}i^2E[X^2]u^2+o(u^2))\\\\\n",
        "&=1+iE[X]u+iE[Y]v+\\frac{1}{2}\\begin{bmatrix}u,v\\end{bmatrix} \\begin{bmatrix}i^2E[X^2] & i^2E[XY]\\\\\n",
        "i^2E[XY] & i^2E[Y^2]\\end{bmatrix}\\begin{bmatrix}u\\\\v\\end{bmatrix}+o(|u^2+v^2|).\\end{align*}$$\n",
        "Thus we have \n",
        "$$(\\widehat{F}-\\widehat{fg})(0,0)=\\frac{1}{2}\\begin{bmatrix}u,v\\end{bmatrix} \\begin{bmatrix}0 & \\mathrm{Cov}(X,Y)\\\\\n",
        "\\mathrm{Cov}(X,Y) & 0\\end{bmatrix}\\begin{bmatrix}u\\\\v\\end{bmatrix}+o(|u^2+v^2|).$$\n",
        "Similarly, we have\n",
        "$$E[e^{iu(X-\\overline{X})}e^{iv(Y-\\overline{Y})}]=1 + \\frac{1}{2}\\begin{bmatrix}u,v\\end{bmatrix} \\begin{bmatrix}\\mathrm{Var}(X) & \\mathrm{Cov}(X, Y)\\\\\n",
        "\\mathrm{Cov}(X, Y) & \\mathrm{Var}(X)\\end{bmatrix}\\begin{bmatrix}u\\\\v\\end{bmatrix}+o(|u^2+v^2|).$$\n",
        "\n",
        "Thus the statement that \"Covariance(correlation) captures the linear dependency\" becomes **natural, intuitive and precise**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4GxoDTm_Lix",
        "colab_type": "text"
      },
      "source": [
        "# Why do we have the covariance inequality?\n",
        "\n",
        "Question: Why do we have the Cauchy inequality $$\\mathrm{Cov}(X,Y)\\leq \\sqrt{\\mathrm{Var}(X)\\mathrm{Var}(Y)}.$$\n",
        "We ask this question because for a random function, we shouldn't expect the terms in the Hessian to satisfy such an inequality. For example $F(x,y)=xy$, then $F_{xy}=1$, $F_{xx}=F_{yy}=0$, we **don't have** $F_{xy}^{2}\\leq F_{xx}F_{yy}$, thus what property of $F$ being a probability distribution gurantees the covariance inequality?\n",
        "\n",
        "By the [**Herglotz theorem**, *Lectures on prediction theory*, Page 2](https://link.springer.com/content/pdf/10.1007%2FBFb0074242.pdf), or more generally [Herglotz-Bochner representation theorem](http://www.orsj.or.jp/~archive/pdf/e_mag/Vol.60_02_122.pdf), it basically comes from the positivity of the distribution function. Namely, $\\frac{1}{2}u^2\\mathrm{Var}+\\frac{1}{2}v^2\\mathrm{Var}(Y)+uv\\mathrm{Cov(XY)}``=\"E[|\\int_{\\mathrm{R}^2}e^{i(u-v)}uv|^2]$(check the references for the exact statement). This implies the determinate of the Hessian of the characteristic function are given by an expectation of a norm square function. This leads to the Cauchy inequality naturally, and there's really no difference between the validity of the ordinary Cauchy's inequality and the covariance inequality. Though it's not hard to prove the covariance ind=equality by the $E[(X+\\mu Y)^2]\\geq 0$ is true for any $\\mu$ trick, but this doesn't explain why it's there in the first place. However, from the characteristic function point of view, the positivity of the Hessian is leterally an incarnation of the fact that $E[X^2]>0$ (not an indirect implication, notice the subtle difference between the ordinary proof and this Fourier transform point of view), which furthur boils down to the simple fact that distances are non-negative. The covariance inequality thus becomes a natural and intuitive statement. \n",
        "\n",
        "But now the determinate of the Hessian is not the modulus of an obvious variable, that's the flavour of all kinds of dualities of transforms. We really see this trade-off here, on the characteristic function side, the positivity is mysterious and very hard to check in general, but easy on the probability side, however, things like zero correlation doesn't imply indenpency is intuitively hard to see on the probability side but very easy to understand on the charactersitic side. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6-eC0_dCiGQ",
        "colab_type": "text"
      },
      "source": [
        "# A simple application of intuition\n",
        "\n",
        "- We have three random variables, $\\mathrm{Cov}(X,Y)>0$, $\\mathrm{Cov}(Y,Z)>0$, is it true that $\\mathrm{Cov}(X,Z)>0$.\n",
        "\n",
        "The answer is intuitively easy, since if we have three vectors, let's say, $A=(1,0)$, $B=(\\frac{1}{2},\\frac{\\sqrt{3}}{2})$ and $C=(\\frac{1}{2}, -\\frac{\\sqrt{3}}{2})$.Then $\\angle AB$ and $\\angle BC$ are acute, but $\\angle AC$ is obtuse. The problem is that **how to translate this to give constructions of random variables?** We have the feeling that the X-direction and $Y$-direction are \"independent\" (later on, this intuition can be more precise), $A$, $B$ are positive correlated because part of $A$ is correlated to $B$ (the $X$-direction), part of $A$ is \"independent\" of $B$ (the $Y$-direction), similar for other pairs. Thus we let $X$, $Y$ be two independent variables, let $A=\\frac{1}{2}X+\\frac{\\sqrt{3}}{2}Y$, $B=X$, $C=\\frac{1}{2}X-\\frac{\\sqrt{3}}{2}Y$, we can even assume that $E[X]=E[Y]=0$. Then it's easy to check $A$ $B$, $C$ violate the transitivity of correlations. \n",
        "\n",
        "However, if we view $X$-direction and $Y$-direction as \"independent\", then how do think of \"zero-correlated\" variables? The idea actually lies in algebraic geometry---$\\mathfrak{m}$-adic topology, or in other words, a point is not just a point, we have all higher-order \"tangency\" conditions. We can even make this precise\n",
        "\n",
        "(Shuai's \"Definition\") We define the **order of independency $\\mathrm{ord}(X,Y)$**  of two random variables to the the degree of the ideal $I_{X,Y}$ generated by all coefficients of $\\widehat{F}-\\widehat{fg}$ minus &1&, the order of the zero ideal is defined to be $+\\infty$. Thus if two random variables have zero-correlation, that exactly means $\\mathrm{ord}(X,Y)\\geq 1$. For two variables to be independent, we need $I_{X,Y}=0$, namely $\\mathrm{ord}(X,Y)=+\\infty$."
      ]
    }
  ]
}